\documentclass[man]{apa}
\usepackage{graphicx,epsfig,amsmath,alltt,setspace,bm}
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, echo = FALSE, results = hide}

%%      4. deal with color in figures
\title{Tests of measurement invariance without subgroups:
  A~generalization of classical methods}
\twoauthors{Edgar C. Merkle}{Achim Zeileis}
\twoaffiliations{University of Missouri}{Universit\"{a}t Innsbruck}

\abstract{
  The issue of measurement invariance commonly arises in factor-analytic
  contexts, with methods for assessment including likelihood ratio tests,
  Lagrange multiplier tests, and Wald tests. These tests all require advance
  definition of the number of groups, group membership, and offending model
  parameters. In this paper, we study tests of measurement invariance based on
  stochastic processes of casewise derivatives of the likelihood function. These
  tests can be viewed as generalizations of the Lagrange multiplier test, and they
  are especially useful for: (1)~identifying subgroups of individuals
  that violate measurement invariance along a continuous auxiliary variable
  without prespecified thresholds, and (2)~identifying specific parameters impacted by
  measurement invariance violations.
  The tests are presented and illustrated in detail, including an application
  to a study of stereotype threat and simulations examining the tests'
  abilities in controlled conditions.
}

\acknowledgements{This work was supported by National Science
  Foundation grant SES-1061334. The authors thank Jelte Wicherts, who
  generously shared data for the stereotype threat application,
  Yves Rosseel, who provided feedback and code for performing the
  tests with the lavaan package, and the participants of the
  Psychoco 2012 workshop on psychometric computing for helpful 
  discussion.
  Correspondence to Edgar C.\ Merkle, Department of  
  Psychological Sciences, University of Missouri, Columbia,
  MO 65211.
  Email: {\texttt{merklee@missouri.edu}}.}
\shorttitle{Tests of measurement invariance without subgroups}
\rightheader{Tests of measurement invariance without subgroups}

\newcommand{\argmax}{\operatorname{argmax}\displaylimits}

%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\spacing{1}

\begin{document}
\maketitle

<<preliminaries>>=
## packages
library("OpenMx")
library("lavaan")
library("strucchange")
library("mvtnorm")
library("lattice")

## auxiliary code
source("mz.R")
source("estfun-lavaan.R")
source("sim.R")

## convenience function for plotting boundaries without color
get_boundary <- function(obj, fun, trim = 0) {
  bound <- fun$boundary(0:obj$nobs/obj$nobs)
  bound <- fun$computeCritval(0.05, ncol(obj$process)) * bound
  bound <- zoo(bound, time(obj$process))
  if(trim > 0) bound <- head(tail(bound, - floor(trim * obj$nobs)), - floor(trim * obj$nobs))
  return(bound)
}
@

The assumption that parameters are
invariant across observations is a
fundamental tenet of many statistical models.  A specific type of 
parameter invariance, measurement invariance, has implications for the
general design and use of psychometric scales.
This concept is 
particularly important because violations can render the scales
difficult to interpret.  
That is, if a set of scales violates measurement invariance,
then individuals with the same ``amount'' of a latent variable 
may systematically receive 
different scale scores.
This may lead researchers to conclude subgroup
differences on a wide variety of interesting constructs 
when, in reality, the scales impact the magnitude of the 
differences.  Further, it can be inappropriate to incorporate scales
violating measurement invariance into structural equation
models, where relationships between latent variables are hypothesized.
\citeA{HorMca92} concisely summarize the impact of
these issues, stating ``Lack of evidence of measurement 
invariance equivocates conclusions and casts doubt on theory in the
behavioral sciences'' (p.~141).  \citeA{Bor06a} further 
notes that researchers often fail to assess whether measurement
invariance holds.

In this paper, we apply a family of statistical tests based on
stochastic processes to the assessment of measurement invariance.
The tests are shown to have useful advantages over existing
tests.  We begin by developing a general framework for the tests,
including discussion of theoretical results relevant to the
proposed tests and comparison of the proposed tests to the 
existing tests.  Next, we study the 
proposed tests' abilities through example and simulation.  Finally,
we discuss some interesting extensions of the tests.
Throughout the manuscript, we use the term {\em{test}} to refer to a
statistical test and the term {\em{scale}} to refer to a psychometric 
test or scale.

\section{Framework}

The methods proposed here are generally relevant to situations where the
$p$-dimensional random variable $X$ with associated observations $\bm{x}_i, i=1,\dots,n$
is specified to arise from a model with density $f(\bm{x}_i; \bm{\theta})$ and
associated joint log-likelihood
\begin{equation} \label{eq:loglik}
  \ell(\bm{\theta}; \bm{x}_1, \dots, \bm{x}_n) ~=~
    \sum_{i = 1}^n \ell(\bm{\theta}; \bm{x}_i) ~=~
    \sum_{i = 1}^n  \log f(\bm{x}_i; \bm{\theta}),
\end{equation}
where ${\bm \theta}$ is some $k$-dimensional parameter vector that characterizes
the distribution. 
The methods are applicable under very general conditions, essentially
whenever standard assumptions for maximum likelihood inference hold (for more
details see below). For the measurement invariance applications
considered in this paper, we employ
a factor analysis model with assumed multivariate normality:
\begin{eqnarray}
    \label{eq:mvndensity}
    f(\bm{x}_i; \bm{\theta}) & = & \frac{1}{(2\pi)^{p/2} |
      \bm{\Sigma}(\bm{\theta}) |^{1/2}} ~ \exp \left\{ -\frac{1}{2}(\bm{x}_i -
    \bm{\mu}(\bm{\theta}))^{\top} \bm{\Sigma}(\bm{\theta})^{-1} (\bm{x}_i -
    \bm{\mu}(\bm{\theta})) \right\}, \\
    \label{eq:caselik}
    \ell(\bm{\theta}; \bm{x}_i) & = & -\frac{1}{2} \left\{
      (\bm{x}_i - \bm{\mu}(\bm{\theta}))^{\top} \bm{\Sigma}(\bm{\theta})^{-1} (\bm{x}_i - \bm{\mu}(\bm{\theta}))
      ~+~ \log | \bm{\Sigma}(\bm{\theta}) | ~+~ p \log(2 \pi) \right\},
\end{eqnarray}
with model-implied mean vector ${\bm{\mu}}({\bm{\theta}})$ and
covariance matrix ${\bm{\Sigma}}({\bm{\theta}})$. As pointed out above,
the assumptions for the tests introduced here do not require this specific
form of the likelihood, but it is presented for illustration due to
its importance in practice.  

Many expositions of factor analysis utilize the likelihood for the sample
covariance matrix (instead of the likelihood of the individual observations $\bm{x}_i$),
employing a Wishart distribution. However, in that approach the sample covariance
matrix can also be disaggregated to the sum of its individual-specific contributions, 
leading essentially to a likelihood like \eqref{eq:mvndensity} with multivariate normal
observations $\bm{x}_i$. This situation is similar to that encountered in structural
equation models with missing data (e.g., \citeNP{Wot00}).

Within the general framework outlined above and under the usual
regularity conditions (e.g., \citeNP{Fer96}), the model parameters
$\bm{\theta}$ can 
be estimated by maximum likelihood (ML), i.e.,
\begin{equation} \label{eq:ml}
  \hat{\bm{\theta}} ~=~ \argmax_{\bm{\theta}} \ell(\bm{\theta}; x_1, \dots, x_n),
\end{equation}
or equivalently by solving the first order conditions
\begin{equation}
    \label{eq:ml1}
  \sum_{i=1}^{n} {\bm s}(\hat{\bm{\theta}}; \bm{x}_i) ~=~ 0,    
\end{equation}
where 
\begin{equation}
  \label{eq:score}
  {\bm s}({\bm \theta}; x_i) ~=~ \left(
    \frac{\partial \ell({\bm \theta}; x_i)}{\partial \theta_1},
    \dots,
    \frac{\partial \ell({\bm \theta}; x_i)}{\partial \theta_k}
  \right)^\top,
\end{equation}
is the score function of the model, i.e., the partial derivative of the casewise
likelihood contributions w.r.t.\ the parameters $\bm{\theta}$.
Evaluation of the score function at $\hat{\bm{\theta}}$ for $i=1,
\dots, n$ essentially measures the extent to which each individual's
likelihood is maximized.

One central assumption -- sometimes made implicitly -- is that
the same model parameters $\bm{\theta}$ hold for all individuals $i = 1, \dots, n$.
If this is not satisfied, the estimates $\hat{\bm{\theta}}$ are typically
not meaningful and cannot be easily interpreted.
One potential source of deviation from this assumption is lack of
measurement invariance, investigated in the following section.



\section{Tests of Measurement Invariance}

In general terms, a set of scales is defined to be measurement
invariant with respect to an auxiliary variable $V$ if:
\begin{equation}
    \label{eq:midef}
      f(x_i | t_i, v_i, \dots) = f({\bf x}_i | t_i, \dots),
\end{equation}
where $x_i$ is the data vector for individual $i$, $t_i$ is the latent
variable for individual $i$ that the scales purport to measure, and
$f$ is the model's 
distributional form \cite{Mel89}.  
We adopt a parametric,
factor-analytic framework here, so that the above equation being true
for all $V$ implies that the measurement
parameters are equal 
across individuals and, thus, do not vary with any
$V$ \cite{Mer93}.

To frame this as a formal hypothesis, we assume that -- in principle --
Model~(\ref{eq:loglik}) holds for all individuals but with a
potentially individual-specific 
parameter vector ${\bm \theta}_i$. The null hypothesis of measurement invariance
is then equivalent to the null hypothesis of parameter constancy
\begin{equation}
    \label{eq:h0}
    H_0:~ {\bm \theta}_i = {\bm \theta}_0,\quad (i=1,\ldots,n),
\end{equation}
which should be tested against the alternative that the parameters
are some nonconstant function ${\bm \theta}(\cdot)$ of the variable $V$ with observations
$v_1, \dots, v_n$, i.e.,
\begin{equation}
    \label{eq:h1}
    H_1:~ {\bm \theta}_i = {\bm \theta}(v_i),\quad (i=1,\ldots,n).
\end{equation}
where the pattern ${\bm \theta}(V)$ of deviation from measurement invariance is
typically not known (exactly) in practice. If it were (see below for
some concrete examples), then standard inference methods -- such as likelihood
ratio, Wald, or Lagrange multiplier tests -- could be employed. However,
if the pattern is unknown, it is difficult to develop a single test
that is well-suited for all conceivable patterns. But it is possible to
derive a family of tests so that representatives from this family are well-suited
for a wide range of possible patterns. One pattern of particular interest
involves $V$ dividing the individuals into two subgroups with different
parameter vectors
\begin{equation}
  \label{eq:h1*}
  H_1^*:~ {\bm \theta}_i = \left\{ \begin{array}{ll}
    {\bm \theta}^{(A)} & \mbox{if } v_i \le \nu, \\
    {\bm \theta}^{(B)} & \mbox{if } v_i >   \nu,
  \end{array} \right.
\end{equation}
where ${\bm \theta}^{(A)} \neq {\bm \theta}^{(B)}$. This could pertain to 
two different age groups, income groups, genders, etc.

Note that even when adopting $H_1^*$ as the relevant alternative, the pattern
${\bm \theta}(V)$ is not completely specified unless the cutpoint $\nu$ is known
in advance (i.e., unless there is observed heterogeneity).  In this situation, all individuals can be grouped based on $V$, and
we can apply standard theory: nested multiple group models (e.g.,
\citeNP{Jor71,Bol89}) coupled with likelihood ratio (LR) tests are most common,
although the asymptotically equivalent  Lagrange multiplier
(LM) tests (also known as score tests) and Wald tests may also be used for this purpose (see
\citeNP{Sat89}). If $\nu$
is unknown (as is often the case for continuous $V$), however, then
there is unobserved heterogeneity and standard theory 
is not easily applied.   Nonstandard inference methods, such as those
proposed in this paper, are then required.

In the following section, we describe the standard approaches to
testing measurement invariance with $\nu$ known.  We then contrast
these approaches with the tests proposed in this paper.
We assume throughout that the observations $i = 1, \dots, n$ are
ordered with respect to the random variable $V$ of interest such that
$v_1 \le v_2 \le \dots \le v_n$.  We also assume that the measurement
model is correctly specified, as is implicitly assumed under traditional
measurement invariance approaches.  In particular, violations of
normality may lead to spurious results in the proposed tests just as
they do in other approaches (e.g., \citeNP{BauCur04}).


\subsection{Likelihood Ratio, Wald, and Lagrange Multiplier Test for Fixed Subgroups}

To employ the LR~test for assessing measurement invariance, model
parameters are estimated separately for a certain number of subgroups
of the data (with some parameters potentially restricted to be equal across
subgroups).  For ease of exposition, we describe
the case where there are no such parameter restrictions; as shown in
the example and simulation below, however, it is
straightforward to extend all methods to the more general case.
After fitting the model to each subgroup, the sum of maximized likelihoods 
from the subgroups are compared with the original maximized full-sample likelihood
in a $\chi^2$~test. For the special case of two subgroups,
the alternative $H_1^*$ from (\ref{eq:h1*}) with fixed and prespecified $\nu$ is adopted
and the null hypothesis $H_0$ from (\ref{eq:h0}) reduces to ${\bm \theta}^{(A)} = {\bm \theta}^{(B)}$.
The parameter estimates $\hat {\bm \theta}^{(A)}$ can then be obtained from the
observations $i = 1, \dots, m$, say, for which $v_i \le \nu$. Analogously,
$\hat {\bm \theta}^{(B)}$ is obtained by maximizing the likelihood for the
observations $i = m + 1, \dots, n$, for which $v_i > \nu$. The LR~test statistic
for the given threshold $\nu$ is then
\begin{equation} \label{eq:lr}
  \mathit{LR}(\nu) ~=~ -2 \left[
         \ell(\hat {\bm \theta}; x_1, \dots, x_n)
   ~-~ \{\ell(\hat {\bm \theta}^{(A)}; x_1, \dots, x_m)
    +    \ell(\hat {\bm \theta}^{(B)}; x_{m+1}, \dots, x_n)\}
    \right],
\end{equation}
which, when \eqref{eq:h1*} holds, has an asymptotic $\chi^2$ with degrees
of freedom equal to the number of parameters in~${\bm \theta}$.

Analogously to the LR~test, the Wald test and LM test
can be employed to test the null hypothesis $H_1^*$ for a fixed threshold $\nu$.
For the Wald test, the idea is to compute the Wald statistic $W(\nu)$ as a quadratic
form in $\hat {\bm \theta}^{(A)} - \hat {\bm \theta}^{(B)}$, utilizing its estimated
covariance matrix for standardization. For the LM test, the LM statistic
$\mathit{LM}(\nu)$ is a quadratic form in ${\bm s}(\hat {\bm \theta}; x_1, \dots, x_m)$
and ${\bm s}(\hat {\bm \theta}; x_{m+1}, \dots, x_n)$. Thus, the three tests all assess
differences that should be zero under $H_0$: for the LR~test the difference of maximized
likelihoods; for the Wald test, the difference of parameter estimates; and for the
LM test, the differences of likelihood scores from zero. In the LR~case, the parameters
have to be estimated under both the null hypothesis and alternative.
Conversely, the Wald case requires  
only the estimates under the alternative, while the LM case requires
only the estimates under the null hypothesis.


\subsection{Extensions for Unknown Subgroups}

For assessing measurement invariance in psychometric models, the major limitation of
the three tests is that the potential subgroups have to be known in advance. 
Even if the scales are known to violate measurement invariance w.r.t.\ the variable $V$, the threshold $\nu$ from (\ref{eq:h1*}) is often unknown in practice.
For example, if $V$ represents yearly income, there are many possible
values of $\nu$ that could be used to divide individuals into poorer and richer
groups.  The ultimate $\nu$ that we
choose could potentially impact our
conclusions about whether or not a scale is measurement invariant, 
in the same way that dichotomization of continuous variables impacts
general psychometric analyses \cite{MacZha02}.

Instead of choosing a specific $\nu$, a natural idea is to compute $\mathit{LR}(\nu)$
for each possible value in some interval $[\underline{\nu}, \overline{\nu}]$ and
reject if their maximum
\begin{equation} \label{eq:maxlr}
  \max_{\nu \in [\underline{\nu}, \overline{\nu}]} \mathit{LR}(\nu)
\end{equation}
becomes large. Note that this corresponds to maximizing the likelihood w.r.t.\
an additional parameter, namely $\nu$. Hence, the asymptotic distribution of the
maximum $\mathit{LR}$ statistic is not $\chi^2$ anymore. \citeA{And93}
showed that the asymptotic distribution is in fact tractable but nonstandard.
Specifically, the asymptotic distribution of (\ref{eq:maxlr}) is the maximum of a
certain tied-down Bessel process whose specifics also depend on the minimal
and maximal thresholds $\underline{\nu}$ and $\overline{\nu}$, respectively.
See Andrews for the original results and further references, and see 
below for more details on the results' application to measurement invariance.

Analogously, one can consider $\max W(\nu)$ and
$\max \mathit{LM}(\nu)$, respectively, which both have the same asymptotic properties
as $\max \mathit{LR}(\nu)$ and are asymptotically equivalent \cite{And93}.
From a computational perspective, the $\max \mathit{LM}(\nu)$ test is
particularly convenient 
because it requires just a single set of estimated parameters $\hat {\bm \theta}$
which is employed for all thresholds $\nu$ in $[\underline{\nu},
\overline{\nu}]$.  
The other two tests require reestimation of the subgroup models for each
$\nu$.

So far, the discussion focused on the alternative $H_1^*$: The maximum LR, Wald, and LM
tests are designed for a situation where there is a single threshold at which all
parameters in the vector ${\bm \theta}$ change. While this is plausible and intuitive
in many applications, it would also be desirable to obtain tests that direct their power
against other types of alternatives, i.e., against $H_1$ with other patterns ${\bm \theta}(V)$.
For example, the parameters may fluctuate randomly or there might be multiple thresholds
at which the parameters change. Alternatively, only one (or just a few) of the parameters in
the vector ${\bm \theta}$ may change while the remaining parameters are
constant (a common occurrence in psychometric models).
To address such situations in a unified way, the next section contains 
a general framework for testing measurement invariance along a
(continuous) variable $V$ that
includes the maximum LM test as a special case.


\section{Stochastic Processes for Measurement Invariance}

As discussed above, factor analysis models are typically
estimated by fitting the model to all $i = 1, \dots, n$ individuals, assuming
that the parameter vector ${\bm \theta}$ is constant across individuals.
Having estimated the parameters $\hat {\bm \theta}$, the goal is to check that
all subgroups of individuals conform with the model (for all 
of the parameters). Hence, some measure of model deviation or residual is required
that captures the lack of fit for the $i$-th individual at the $j$-th parameter
($i = 1, \dots, n$, $j = 1, \dots, k$). A natural measure -- that employs the ideas
of the LM test -- is ${\bm s}(\hat {\bm \theta}; x_i)_j$: the $j$-th component of the
contribution of the $i$-th observation to the score function. By construction, the sum
of the score contributions over all individuals is zero for each
component; see
(\ref{eq:ml1}). Moreover, if there are no systematic deviations, the score contributions
should fluctuate randomly around zero.  Conversely, the score
contributions should be shifted away from zero for
subgroups where the model does not fit.

Therefore, to employ this quantity for tests of measurement invariance against alternatives
of type (\ref{eq:h1}), we need to overcome two obstacles: (1)~make use of the ordering of
the observations w.r.t.\ $V$ because we want to test for changes ``along'' $V$; (2)~account
for potential correlations between the $k$~components of the parameters to be able to detect
which parameter(s) change (if any).

\subsection{Theory}

The test problem of the null hypothesis (\ref{eq:h0}) against the alternatives
(\ref{eq:h1}) and (\ref{eq:h1*}), respectively, has been studied extensively
in the statistics and econometrics literature under the label ``structural change
tests'' (see e.g., \citeNP{BroDur75,And93}) where the focus of interest is the
detection of parameter instabilities of time series models ``along'' time.
Specifically, it has been shown (e.g., \citeNP{Nyb89,Han92,HjoKon02,ZeiHor07})
that cumulative sums of the empirical scores follow specific stochastic processes, allowing
us to use them to generally test measurement invariance. Here, we review
some of the main results from that literature and adapt it to the specific challenges
of factor analysis models. More detailed accounts of the underlying structural
change methods include \citeA{HjoKon02} and \citeA{ZeiHor07}.

For application to measurement invariance, the most important
theoretical result involves the fact that, under the $H_0$ in \eqref{eq:h0}, the
\emph{cumulative score process} converges to a specific asymptotic
process. The $k$-dimensional cumulative score process is defined as
\begin{equation} \label{eq:cumscore}
  {\bm B}(t; \hat {\bm \theta}) ~=~ \hat {\bm I}^{-1/2} n^{-1/2}
    \sum_{i = 1}^{\lfloor n t \rfloor} {\bm s}(\hat {\bm \theta}; x_i)
  \qquad (0 \le t \le 1)
\end{equation}
where $\lfloor nt \rfloor$ is the integer part of $nt$ and
$\hat I$ is some consistent estimate of the covariance matrix of the scores,
e.g., their outer product or the observed information matrix.
The $k$~dimensions of the 
process arise from the fact that a separate cumulative score is
maintained for each of the $k$ model parameters.  
As the equation shows, the cumulative score process adds subsets of
casewise score contributions across individuals along the ordering
w.r.t.\ the variable $V$ of interest.  At $t=1/n$, only the
first individual's contribution enters into the summation; at
$t=2/n$, the first two individuals' contributions enter into the
summation, etc., until $t = n/n$
where all contributions enter into the summation. Thus, due to (\ref{eq:ml1}),
the cumulative score process always equals zero at $t = 0$ and
returns to zero at $t = 1$. Furthermore, multiplication by
$\hat {\bm I}^{-1/2}$ ``decorrelates'' the $k$~cumulative score
processes, such that each univariate process (i.e., each
process for a single model parameter) is unrelated to (and asymptotically independent of) all other
processes. Therefore, this cumulative process ${\bm B}(t; \hat {\bm \theta})$
accomplishes the challenges discussed at the beginning of this section:
it makes use of the ordering of the observations by taking cumulative sums,
and it decorrelates the contributions of the $k$ different parameters.

Inference can then be based on an extension of the usual central limit
theorem. Under the assumption of independence of individuals (implicit
already in Equation~\ref{eq:loglik}) and under
the usual ML regularity conditions (assuring asymptotic normality
of $\hat {\bm \theta}$), \citeA{HjoKon02} show
that 
\begin{equation} \label{eq:fclt}
  {\bm B}(\cdot; \hat {\bm \theta}) ~\overset{d}{\rightarrow}~ {\bm B}^{0}(\cdot),
\end{equation}
where $\overset{d}{\rightarrow}$ denotes convergence in distribution
and ${\bm B}^{0}(\cdot)$ is a $k$-dimensional Brownian bridge.
In words, there are $k$ cumulative score processes, one for each
model parameter.  This collection of processes follows a
multidimensional Brownian bridge as score contributions accumulate in the 
summation from individual 1 (with lowest value of $V$) to individual $n$
(with highest value of $V$).

The empirical cumulative score process from (\ref{eq:cumscore}) can also be viewed
as an $n \times k$ matrix with elements ${\bm B}(i/n; \hat {\bm \theta})_j$ that
we also denote ${\bm B}(\hat {\bm \theta})_{ij}$ below for brevity.
That is, assuming that individuals are ordered by $V$, the first row
of ${\bm B}(\hat {\bm \theta})$ corresponds to the first individual's
scores.  The second row of ${\bm B}(\hat {\bm \theta})$ corresponds to
the sum of the first two individuals' scores, etc., until the last row
of ${\bm B}(\hat {\bm \theta})$ corresponds to the sum of all
individuals' scores (which will be a row of zeroes).
Under this setup, each column of ${\bm B}(\hat {\bm \theta})$
converges to a univariate Brownian bridge and pertains 
to a single factor analysis parameter. To carry out a test of $H_0$, the process/matrix
needs to be aggregated to a scalar test statistic by collapsing across
rows (individuals) and columns (parameters) of the matrix. The asymptotic distribution
of this test statistic is then easily obtained by applying the same
aggregation to the asymptotic Brownian bridge \cite{HjoKon02,ZeiHor07},
so that corresponding $p$-values can be derived.

As argued above, no single aggregation function will have high power for all
conceivable patterns of measurement invariance ${\bm \theta}(V)$, while any
(reasonable) aggregation function will have non-trivial power under $H_1$.
Thus, various aggregation strategies should be employed depending on which
pattern ${\bm \theta}(V)$ is most plausible (because the exact pattern
is typically
unknown). A particularly agnostic aggregation strategy is to reject $H_0$
if any component of the the cumulative score process ${\bm B}(t; \hat {\bm \theta})$
strays ``too far'' from zero at any time, i.e., if
\begin{equation}
    \label{eq:dmax}
    \mathit{DM} = \max_{i = 1,\dots, n} \max_{j = 1, \dots, k} | {\bm B}(\hat {\bm \theta})_{ij} |,
\end{equation}
becomes large.  In determining where in ${\bm B}(\hat {\bm \theta})$
this maximum occurred, we are able to determine  
threshold(s) of parameter change (over the individuals $i = 1, \dots, n$)
and the parameter(s) affected by it (over $j = 1, \dots, k$). This test is
especially useful for visualization, as the cumulative score process for
each individual parameter can be displayed along with the appropriate
critical value. For an example of this see Figure~\ref{fig:lambdas}
(in the ``Example'' section) which shows the the cumulative score processes for three
factor loadings along with the critical values at 5\% level.

However, taking maximums ``wastes'' power if many of the $k$~parameters change
at the same threshold, or if the score process takes large values for
many of the $n$~individuals (and not just a single threshold). In such
cases, sums insteads of maxima are more suitable for collapsing across
parameters and/or individuals, because they combine the deviations instead of
picking out only the single largest deviation. Thus, if the parameter instability
${\bm \theta}(V)$ affects many parameters and leads to many subgroups, sums
of (absolute or squared) values should be used for collapsing both across
parameters and individuals. On the other hand, if there is just a single
threshold that affects multiple parameters, then the natural aggregation
is by sums over parameters and then by the maximum over individuals.
More precisely, the former idea leads to a Cram{\'e}r-von Mises type
statistic and the latter to the maximum LM statistic from the previous section:
\begin{eqnarray}
    \label{eq:cvm}
    \mathit{CvM}     & = & n^{-1} \sum_{i = 1,\dots, n} \sum_{j = 1, \dots, k} {\bm B}(\hat {\bm \theta})_{ij}^2, \\
    \label{eq:maxlm}
    \max \mathit{LM} & = & \max_{i = \underline{i}, \dots, \overline{\imath}} ~
      \left\{ \frac{i}{n} \left( 1 - \frac{i}{n} \right) \right\}^{-1}
      \sum_{j = 1, \dots, k} {\bm B}(\hat {\bm \theta})_{ij}^2,
\end{eqnarray}
where the $\max \mathit{LM}$ statistic is additionally scaled by the
asymptotic variance $t (1 - t)$ of the process ${\bm B}(t, \hat {\bm \theta})$.
It is equivalent to the $\max_\nu \mathit{LM}(\nu)$ statistic from the previous
section, provided that the boundaries for the subgroups sizes $\underline{i}$/$\underline{\nu}$
and $\overline{\imath}$/$\overline{\nu}$ are chosen analogously.
Further aggregation functions have been suggested in the structural
change literature (see e.g., \citeNP{Zei05,ZeiSha10}) but the three
tests above are most likely to be useful in psychometric settings.

Finally, all tests can be easily modified to address the situation of
so-called ``partial structural changes'' \cite{And93}. This refers to the case of some
parameters being known to be stable, i.e., restricted to be constant across
potential subgroups. Tests for potential changes/instabilities only in the
$k^*$ remaining parameters (from overall $k$ parameters) are easily obtained
by omitting those $k - k^*$ columns from ${\bm B}(\hat {\bm \theta})_{ij}$ that are
restricted/stable, retaining only those $k^*$ columns that are potentially instable.
This may be of special interest to those wishing to test specific types of measurement
invariance, where subsets of model parameters are assumed to be stable.


\subsection{Critical Values \& $p$-values}

As pointed out above, specification of the asymptotic distribution under $H_0$ for the test
statistics from the previous section is straightforward: it is simply the aggregation
of the asympotic process ${\bm B}^0(t)$ \cite{HjoKon02,ZeiHor07}. Thus, $\mathit{DM}$
from (\ref{eq:dmax}) converges to $\sup_t || {\bm B}^0(t) ||_{\infty}$, where
$||\cdot||_\infty$ denotes the maximum norm. Similarly, $\mathit{CvM}$
from (\ref{eq:cvm}) converges to $\int_0^1 || {\bm B}^0(t) ||_2^2 d t$, where $||\cdot||^2$
denotes the Euclidean norm. Finally, $\max \mathit{LM}$ from (\ref{eq:maxlm}) -- and analogously
the maximum Wald and LR~tests -- converges to $\sup_t (t (1-t))^{-1} || {\bm B}^0(t) ||_2^2$
(which can also be interpreted as the maximum of a tied-down Bessel process, as pointed out
previously).

While it is easy to formulate these asymptotic distributions theoretically,
it is not always easy to find closed-form solutions for computing critical
values and $p$-values from them. In some cases -- in particular for the double maximum test --
such a closed-form solution is available from analytic results for Gaussian
processes (see e.g., \citeNP{ShoWel86}). For all other cases, tables of critical
values can be obtained from direct simulation \cite{Zei06} or in combination with
more refined techniques such as response surface regression \cite{Han97}.

The analytic solution for the asymptotic $p$-value of a $\mathit{DM}$ statistic
$d$ is
\begin{equation}
    \label{eq:absmax_p}
        P(\mathit{DM} > d ~|~ H_0) ~\overset{asy}{=}~ 1 - \left\{1 ~+~ 2 \sum_{h = 1}^{\infty} (-1)^h \exp(-2 h^2 d^2) \right\}^k.
\end{equation}
This combines the crossing probability of a univariate Brownian bridge
(see e.g., \citeNP{ShoWel86,PloKra92}) with a straightforward Bonferroni
correction to obtain the $k$-dimensional case. The terms in the summation
quickly go to zero as $h$ goes to infinity, so that only some large finite
number of terms (say, 100) need to be evaluated in practice.

For the Cram\'{e}r-von Mises test statistic $\mathit{CvM}$, \citeA{Nyb89}
and \citeA{Han92} provide small tables of critical values which
have been extended in the software provided by \citeA{Zei06}.
Critical values for the distribution of the maximum LR/Wald/LM tests
are provided by \citeA{Han97}. Note that the distribution depends on
the minimal and maximal thresholds employed in the test.

\subsection{Local Alternatives}
Using results from \citeA{HjoKon02} and \citeA{ZeiHor07}, we can also
capture the behaviour of the process $\bm{B}(t, \hat{\bm{\theta}})$
under specific alternatives 
of parameter instability. In particular, we can assume that the pattern
of deviation $\bm{\theta}(v_i)$ can be described as a constant
parameter plus some non-constant deviation $\bm{g}(i/n)$:
\begin{equation}
    \label{eq:local}
  \bm{\theta}(v_i) = \bm{\theta}_0 + n^{-1/2} \bm{g}(i/n).
\end{equation}
In this case, the scores $\bm{s}(\bm{\theta}_0; x_i)$ from
Equation~\eqref{eq:score} do not have zero expectation but rather
\begin{equation}
    \label{eq:localE}
  E[\bm{s}(\bm{\theta}_0; x_i)] = \bm{0} + n^{-1/2}
  \bm{C}(\bm{\theta}_0) \bm{g}(i/n).
\end{equation}
The covariance matrix $\bm{C}(\bm{\theta})$ from the expected
outer product of gradients is then 
\begin{equation}
    \label{eq:localS}
  \bm{C}(\bm{\theta}_0) = E[\bm{s}(\bm{\theta}_0; x_i) \bm{s}(\bm{\theta}; x_i)']    
\end{equation}
which, at $\bm{\theta}_0$, coincides with the expected information matrix.

Under the local alternative described above, \citeA{HjoKon02} show that the process
$\bm{B}(t, \hat{\bm{\theta}})$ behaves asymptotically like
\begin{equation}
    \label{eq:localB}
    \bm{B}^0(t) + \hat{\bm{I}}^{-1/2} \bm{C}(\hat{\bm{\theta}}) \bm{G}^0(t),
\end{equation}
i.e., a zero-mean Brownian bridge plus a term with non-zero mean driven by
$\bm{G}^0(t) = \bm{G}(t) - t \bm{G}(1)$, where $\bm{G}(t) = \int_0^t
\bm{g}(y) dy$.  Hence, unless the local
alternative has $\bm{g}(t) \equiv \bm{0}$, the empirical process $\bm{B}(t,
\hat{\bm{\theta}})$ will 
have a non-zero mean.  Hence, the corresponding tests will have non-trivial
power (asymptotically).  We use these results in the ``Simulation''
section to describe the expected behavior of the Brownian bridge.


\subsection{Locating the Invariance}

If the employed parameter instability test detects a measurement invariance violation,
the researcher is typically interested in identification of the parameter(s) affected
by it and/or the associated threshold(s). As argued above, the double maximum test
is particularly appealing for this because the $k$-dimensional empirical cumulative
score process can be graphed along with boundaries for the associated critical values
(see Figure~\ref{fig:lambdas}).
Boundary crossing then implies a violation of measurement invariance, and the location of
the most extreme deviation(s) in the process convey threshold(s) in the underlying
ordering $V$.

For the maximum LR/Wald/LM tests, it is natural to graph the sequence of LR/Wald/LM
statistics along $V$, with a boundary corresponding to the critical value
(see Figure~\ref{fig:lrlm} and the third row of Figure~\ref{fig:cusumex}).
Again, a boundary crossing signals a significant violation, and the peak(s) in the
sequence of statistics conveys threshold(s). Note that, due to summing over all
parameters, no specific parameter can be identified that is responsible for the
violation. Similarly,  neither component(s) nor threshold(s)
can be formally identified for the Cram\'{e}r-von Mises test. However, graphing of (transformations of) the cumulative
score process may still be valuable for gaining some insights (see,
e.g., the second row of Figure~\ref{fig:cusumex}).

If a measurement invariance violation is detected by any of the tests,
one may want to incorporate it into the model to account for it. The
procedure for doing this  typically depends on the type of violation ${\bm \theta}(V)$, and the visualizations
discussed above often prove helpful in determining a suitable parameterization.
In particular, one approach that is often employed in practice
involves adoption of a model
with one (or more) threshold(s) in all parameters (i.e., (\ref{eq:h1*}) for the
single threshold case). In the multiple threshold case, their location can
be determined by maximizing the corresponding segmented log-likelihood over all
possible combinations of thresholds (\citeNP{ZeiSha10}, adapt a dynamic programming
algorithm to this task). For the single threshold case, this reduces to
maximizing the segmented log-likelihood
\begin{equation} \label{eq:seglik}
  \ell(\hat {\bm \theta}^{({A)}}; x_1, \dots, x_m) ~+~ \ell(\hat {\bm \theta}^{({B)}}; x_{m+1}, \dots, x_n)
\end{equation}
over all values of $m$ corresponding to possible thresholds $\nu$ (such that $v_m \le \nu$ and
$v_{m+1} > \nu$). As pointed out previously, this is equivalent to maximizing the LR~statistic
from (\ref{eq:lr}) (with some minimal subgroup size typically imposed).

Formally speaking, the maximization of (\ref{eq:seglik}) -- or equivalently (\ref{eq:lr}) --
yields an estimate $\hat \nu$ of the threshold in $H_1^*$. If $H_1^*$ is in fact the true model,
the peaks in the Wald/LM sequences and the cumulative score process, respectively, will occur at the
same threshold asymptotically. However, in empirical samples, their location may differ
(although often not by much).

These attributes give the proposed tests important advantages over
existing tests, as existing measurement invariance methods cannot:
(1)~test measurement invariance for unknown $\nu$, or
(2)~isolate specific parameters violating measurement invariance.
In particular, \citeA{Mil05} cites
``locating the invariance violation'' as a major outstanding problem
in the field.  In the following sections, we demonstrate the proposed
tests' uses and properties via example and simulation.  We first
describe an example and simulation with artificial data, and we then
provide an illustrative example with real data.  Finally, we conclude
the paper with a discussion of test extensions and general summary.

\section{Example with Artificial Data}

\begin{figure}
\caption{Path diagram representing the base factor analysis model used for
  the artificial example and simulations.  To induce measurement invariance
  violations, a seventh observed variable (student age) determines the
values of the verbal factor loadings ($\lambda_{11}$, $\lambda_{21}$,
$\lambda_{31}$).}
\label{fig:famod}
\includegraphics[height=5in]{famod.pdf}
\end{figure}

Consider a hypothetical battery of six scales administered to students aged 13 to 18
years.  Three of the scales are intended to measure verbal ability, and
three of the scales are intended to measure mathematical ability.  We
may observe a maturation effect in the resulting data, whereby the
factor loadings for older students are larger than those for younger
students.  The researcher's goal is to
study whether the scales are measurement invariant with
respect to age, which is taken to be the auxiliary variable $V$.

\subsection{Method}

To formally represent these ideas, we specify that the data arise from
a factor analysis model with two factors.  The base model, displayed in
Figure~\ref{fig:famod}, specifies that measurement invariance holds,
with three scales arising from the verbal factor and three 
scales arising from the mathematical factor.
For the measurement invariance violation, we specify that $V$
(student age) impacts the values of 
verbal factor loadings in the model: 
if students are 16 through 18 years of age, then the factor
loadings corresponding to the first 
factor ($\lambda_{11}, \lambda_{21}, \lambda_{31}$) reflect those in
Figure~\ref{fig:famod}.  If students are 13 through 15 years of age,
however, then the factor loadings corresponding to the first factor
are three standard errors ($=$ asymptotic standard errors divided by $\sqrt{n}$)
lower than those in Figure~\ref{fig:famod}.
This violation states that the verbal ability scales
lack measurement invariance with respect to age.  For simplicity, we
assume that the mathematical scales are invariant.

<<dgp>>=
set.seed(1090)
d <- dgp(200, 2)
@


A sample of size 200 was generated from the model described above, and
a test was conducted to examine measurement invariance of the three
verbal scales.  To carry out
the test, a confirmatory factor analysis model (with the paths
displayed in Figure~\ref{fig:famod}) was fit to the
data.  Casewise derivatives and 
the observed information matrix were then obtained, and they were used
to calculate the cumulative score process via
\eqref{eq:cumscore}.  Finally, we
obtained various test statistics and $p$-values from the cumulative score
process.  These include the double-max statistic from \eqref{eq:dmax},
the Cram\'{e}r-von Mises statistic from \eqref{eq:cvm}, and the $\max \mathit{LM}$
statistic from \eqref{eq:maxlm}.

As mentioned in the theory section, the 
tests give us the flexibility to study hypotheses of partial change.
That is, we have the ability to 
test various subsets of 
parameters.  For example, if we suspected that the verbal factor loadings
lacked measurement invariance, we could test
\begin{equation}
    \label{eq:simh0}
H_0:\ (\lambda_{i,11}\ \lambda_{i,21}\ \lambda_{i,31}) =
(\lambda_{0, 11}\ \lambda_{0, 21}\ \lambda_{0, 31}),\ i=1,\ldots,n,
\end{equation}
where $(\lambda_{i,11}\ \lambda_{i,21}\ \lambda_{i,31})$ represent the
verbal factor loading parameters for student $i$. Thus, here only $k^* = 3$
from the overall $k = 19$~model parameters are assessed (where the 19 parameters include six factor loadings, six unique variances, six intercepts, and one factor correlation).
Alternatively, we can consider all $k^* = k = 19$~parameters, leading to a test
of~\eqref{eq:h0}.  We consider both of these tests below.

\subsection{Results}

Here, we first describe overall results and subsequently
the identification of $\nu$ and isolation of model parameters violating
measurement invariance.

<<mz_mod>>=
if(file.exists("mz_mod.rda")) {
  load("mz_mod.rda")
} else {
  ## full sample model
  mz0 <- mzfit(d)
  ## subsample models
  lrstat <- function(age) 2 * as.vector(logLik(mzfit(d[d$age <= age,])) + logLik(mzfit(d[d$age > age,])) - logLik(mz0))
  mz.age <- sort(d$age[d$age >= quantile(d$age, 0.1) & d$age <= quantile(d$age, 0.9)])
  mz.lrstat <- sapply(mz.age, lrstat)
  save(mz0, mz.age, mz.lrstat, file = "mz_mod.rda")
}
@

<<gefp-3-19>>=
## empirical fluctuation processes (3 vs all 19 pars)
gefp_3_info <- gefp(mz0, fit = NULL, order.by = d$age, parm = 1:3,
  vcov = info.mzfit, sandwich = FALSE)
gefp_19_info <- gefp(mz0, fit = NULL, order.by = d$age,
  vcov = info.mzfit, sandwich = FALSE)
@

\subsubsection{Overall Results}
Test statistics for the hypotheses \eqref{eq:simh0} and~\eqref{eq:h0}
are displayed in Figure~\ref{fig:cusumex}. Each panel displays a test 
statistic's fluctuation across values of student age, with the first
column containing tests of~\eqref{eq:simh0} and the second column
containing tests of~\eqref{eq:h0}.  The solid
horizontal 
lines represent critical values for $\alpha= 0.05$, and
the Cram\'{e}r-von Mises panels 
also contain a dashed line depicting the value of the test
statistic (test statistics for the others are simply 
the maxima of the processes).
In other words, for panels in the first and third rows, \eqref{eq:simh0} is
rejected if the process crosses the horizontal line.  For panels in
the second row, \eqref{eq:h0} is rejected if the dashed horizontal line is
higher than the solid horizontal line.

The figures convey information about several properties
of the tests. First, all three tests are more powerful (and in
this example significant) if we test only those parameters 
that are subject to instabilities. Conversely, if all 19~model
parameters are assessed (including those that are in fact invariant),
the power is decreased. This decrease, however, is less pronounced
for the double-max test as it is more 
sensitive to fluctuations among a small subset of parameters
(3 out of 19 here).

\begin{figure}
\caption{A comparison of the $\max \mathit{LM}$ (solid line) and $\max \mathit{LR}$ (dashed line) test
  statistics for~\eqref{eq:h0} (i.e., $k^* = 19$).  The gray horizontal line
  corresponds to the critical value at $\alpha= 0.05$ while the dotted vertical
  line highlights the threshold at which both test statistics assume their maximum.}
\label{fig:lrlm}
\setkeys{Gin}{width=\textwidth}
<<lm-lr-19, fig=TRUE, height=5, width=6>>=
plot(gefp_19_info, functional = supLM(0.1),
  xlab = "Age", ylab = "LR and LM statistics (k* = 19)",
  main = "", ylim = c(0, 50), boundary = FALSE)
legend("topleft", c("LR", "LM"), lty = c(2, 1), bty = "n")
lines(mz.age, mz.lrstat, lty = 2)
lines(get_boundary(gefp_19_info, supLM(0.1), 0.1), col = "slategray", lwd = 1.7)
abline(v = mz.age[which.max(mz.lrstat)], lty = 3)
@
\end{figure}

Figure~\ref{fig:lrlm} 
compares the $\max \mathit{LM}$ statistic (solid line) to the $\max \mathit{LR}$
statistic (dashed line) from \eqref{eq:maxlr}, as applied to testing
\eqref{eq:h0}. (Note also that the visualization of the $\max \mathit{LM}$ in
Figure~\ref{fig:lrlm} is identical to the bottom right panel of Figure~\ref{fig:cusumex}.)
The critical values for these two
tests are identical, hence the single horizontal line.
The figure
shows that the two statistics are very similar to one another, with
both maxima at the dotted vertical line.  This
is generally to be expected, because the two tests are asymptotically
equivalent.  The 
$\max \mathit{LR}$~statistic cannot be obtained from the empirical fluctuation
process, however, so the factor analysis model must be refitted before
and after each of the possible threshold values $\nu$ (i.e., here
$\Sexpr{2 * length(mz.lrstat)}$~model fits for $\Sexpr{length(mz.lrstat)}$~thresholds).


\subsubsection{Interpretation of Test Results}
As described above, the tests of~\eqref{eq:simh0} imply that the 
verbal scales lack measurement invariance.  
We can also use the tests to: (1)~locate the threshold $\nu$, and
(2)~isolate specific parameters that violate measurement invariance.  
For example, as described previously, information about the location of $\nu$
can be obtained by examining the peaks in Figure~\ref{fig:cusumex}.  For all six panels
in the figure, the peaks occur near an age of 16.1.  This agrees well
with the true threshold of 16.0.

\begin{figure}
\centering
\caption{Cumulative score processes for each verbal factor loading. 
  The solid gray horizontal lines correspond to the critical value of the
  double-max test at $\alpha= 0.05$.}
\label{fig:lambdas}
\setkeys{Gin}{width=0.8\textwidth}
<<gefp-3-visualization, fig=TRUE, height=8, width=5.5>>=
gefp_3_bound <- get_boundary(gefp_3_info, maxBB)
plot(gefp_3_info$process,  ylim = c(-2, 2),
  main = "DM, k* = 3", xlab = "Age", ylab = expression(lambda[11], lambda[21], lambda[31]),
  panel = function(x, y, ...) {
    abline(h = 0)
    lines(gefp_3_bound, col = "slategray", lwd = 1.7)
    lines(-gefp_3_bound, col = "slategray", lwd = 1.7)
    lines(x, y, ...)
  })
@
\end{figure}

We previously noted that the double-max test is advantageous because
it yields information about individual parameters violating measurement invariance.  
That is, it allows us to examine whether or not individual parameters' 
cumulative score processes lead one to reject the hypothesis of
measurement invariance.
Figure~\ref{fig:lambdas} shows the individual cumulative score processes for the
three verbal factor loadings and the top left panel of Figure~\ref{fig:cusumex} shows
the same processes aggregated over the three parameters. In both graphics, horizontal lines reflect the
same critical value at $\alpha= 0.05$. The figure
shows that the third parameter (i.e., $\lambda_{31}$) crosses the
dashed line, so we would conclude a measurement 
invariance violation with respect to age for the third
verbal test.  The fact that the cumulative score process for the first
and second loadings did not achieve the critical value represents a type~II
error, which implicitly brings into question the tests' power.
% This is an example of a situation where the double-max test ``wastes''
% power, as it cannot make use of the fact that multiple model
% parameters change simultaneously.
We
generally address the issue of power in the simulations below.

\begin{figure}
\caption{Three test statistics of~\eqref{eq:simh0} (with $k^* = 3$)
  and~\eqref{eq:h0} (with $k^* = 19$), 
  based on the example involving measurement invariance with respect
  to student age.  Solid gray horizontal lines represent critical values
  at $\alpha = 0.05$, and the dotted, horizontal lines (second row)
  represent values of the Cram\'{e}r-von Mises test statistic.}
\label{fig:cusumex}
\setkeys{Gin}{width=\textwidth}
<<gefp-3-19-visualization, fig=TRUE, height=8, width=6>>=
par(mfcol = c(3, 2))
plot(gefp_3_info,  functional = maxBB, main = "DM, k* = 3", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_3_info, maxBB), col = "slategray", lwd = 1.7)
plot(gefp_3_info,  functional = meanL2BB, main = "CvM, k* = 3", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_3_info, meanL2BB), col = "slategray", lwd = 1.7)
plot(gefp_3_info,  functional = supLM(0.1), main = "max LM, k* = 3", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_3_info, supLM(0.1), 0.1), col = "slategray", lwd = 1.7)
plot(gefp_19_info, functional = maxBB, main = "DM, k* = 19", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_19_info, maxBB), col = "slategray", lwd = 1.7)
plot(gefp_19_info, functional = meanL2BB, main = "CvM, k* = 19", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_19_info, meanL2BB), col = "slategray", lwd = 1.7)
plot(gefp_19_info, functional = supLM(0.1), main = "max LM, k* = 19", xlab = "Age", boundary = FALSE)
lines(get_boundary(gefp_19_info, supLM(0.1), 0.1), col = "slategray", lwd = 1.7)
@
\end{figure}


\section{Simulation}

In this section, we conduct a simulation designed to
examine the tests' power and type~I error rates in situations where
measurement invariance violations are known to exist.  Specifically,
we generated data from the factor analysis model used in the previous
section, with measurement invariance violations in the
``verbal'' factor loadings with respect to a continuous auxiliary variable.

We examine the power and error rates of the three
tests used in the previous section: the double-max test, the
Cram\'{e}r-von Mises test, and the $\max \mathit{LM}$ test.  We also compare 
tests involving only the $k^* = 3$ factor loadings lacking invariance
 with tests of all
$k^* = k = 19$ model parameters.  It is likely that power
is higher when testing only the affected model parameters, but
the affected parameters are usually unknown in practice.  Thus, the
$k^*=3$ and $k^*=19$ conditions reflect boundary cases, with the
performance of tests involving other subsets of model parameters
(e.g., all factor loadings) falling in between the boundaries.  
Finally, we also examine the tests' power across
various magnitudes of measurement invariance violations.

\subsection{Method}
Data were generated from the same model and with the same type of
measurement invariance as was described in the ``Artificial Example'' section.
Sample size and magnitude
of measurement 
invariance violation were manipulated to examine power:
we examined power to detect 
invariance violations across four sample sizes ($n=50, 100,
200, 500$) and 17~magnitudes of violations.  These violations
involved the younger students' values of $\{\lambda_{11},
\lambda_{21}, \lambda_{31}\}$ deviating from the older students'
values by $d$ 
times the parameters' asymptotic standard errors (scaled by $\sqrt{n}$),
with $d = 0, 0.25, 0.5, \dots, 4$.  The 0-standard error condition was
used to study type~I error rate.

For each combination of sample size ($n$) $\times$
violation magnitude ($d$) $\times$ number of parameters being tested
($k^{\ast}$),
5,000 datasets were generated and tested.  In each dataset, half the
individuals had ``low $V$'' (e.g., 13--15 
years of age) and half had ``high $V$'' (e.g., 16--18 years of age).

Using results discussed previously in the ``Local Alternatives''
section, we can derive the expected behavior of the univariate 
Brownian bridges for the parameters $\{\lambda_{11},
\lambda_{21}, \lambda_{31}\}$.  For these parameters, 
we use a simple step function $g(t) = \text{I}(t > 0.5)$ multiplied
by the violation magnitude.  Thus, $G(t) = 
\text{I}(t > 0.5) \times (t - 0.5)$ and $G^0(t) = \{\text{I}(t > 0.5) - 0.5 \} \times t - \text{I}(t > 0.5) \times 0.5$,
both again multiplied by the violation magnitude. This implies that
the mean under the 
alternative is driven by a function that is triangle-shaped, with a peak at the
changepoint $t = 0.5$ for the parameters affected by the change (and
equal to zero for all remaining parameters). 

<<mz_sim>>=
if(file.exists("mz_sim.rda")) {
  load("mz_sim.rda")
} else {
  ## seed for replication
  RNGkind(kind = "default", normal.kind = "default")
  set.seed(1090)
  ## run simulation
  mz_sim <- simulation()
  save(mz_sim, file = "mz_sim.rda")
}

## select only subset
mz_sim_sub <- subset(mz_sim, pars %in% c(3, 19))
mz_sim_sub$pars <- factor(mz_sim_sub$pars)
## labeling
levels(mz_sim_sub$pars) <- paste("k* =", levels(mz_sim_sub$pars))
levels(mz_sim_sub$nobs) <- paste("n =", levels(mz_sim_sub$nobs))
@


\subsection{Results}
Full simulation results are presented in Figure~\ref{fig:simres}, and
the underlying numeric values for a subset of the results is additionally displayed in
Table~\ref{tab:simres}.  In describing the results, we largely refer
to the figure.

Figure~\ref{fig:simres} displays power curves as a function of violation
magnitude, with panels for each combination of sample size ($n$) $\times$
number of parameters being tested ($k^{\ast}$).  
Separate curves are drawn for the double-max test (solid lines), 
the Cram{\'e}r-von Mises test (dashed lines), and the $\max
\mathit{LM}$ test (dotted lines).
One can generally observe that simultaneous tests of all 19~parameters
result in decreased power, with the tests performing more similarly at
the larger sample sizes.  The tests distinguish themselves from one
another when only the three factor loadings are tested, with the
Cram\'{e}r-von Mises test having the most power, followed by $\max \mathit{LM}$,
followed by the double-max test.  
This advantage decreases at larger sample sizes, and we also surmise
that it decreases as extra parameters satisfying measurement
invariance are included in the tests.  
At $n=50$, generally regarded as a
small sample size for factor analysis, the power functions break down
and are not monotonic with respect to violation magnitude.

Table~\ref{tab:simres} presents a subset of the results displayed in 
Figure~\ref{fig:simres}, but it is easier to see exact power
magnitudes in the table.
The table shows that the
power advantage of the Cram\'{e}r-von
Mises test can be as large as 0.1, most notably when three parameters
are being tested. It also shows that the Cram\'{e}r-von Mises test generally
is very close to its nominal type~I error rate, with the double-max
test being somewhat 
conservative and the $\max \mathit{LM}$ test being slightly liberal.

In summary, we found that the proposed tests have adequate power to detect
measurement invariance violations in applied contexts.  The 
Cram\'{e}r-von Mises statistic exhibited the best performance for the
data generated here, though more simulations are warranted to examine
the generality of this finding in other models or other parameter constellations.
In the discussion, we describe extensions of the tests in factor
analysis and beyond.

\begin{figure}
\caption{Simulated power curves for the double-max test (solid), 
  Cram\'{e}r-von Mises test (dashed), and $\max \mathit{LM}$ test (dotted)
  across four sample sizes $n$, two subsets of tested parameters $k^*$, and
  measurement invariance violations of 0--4 standard errors (scaled by $\sqrt{n}$). 
  See Table~\ref{tab:simres} for the underlying numeric values (using a
  subset of nine violation magnitudes).}
\label{fig:simres}
\setkeys{Gin}{width=\textwidth}
<<mzsim-xyplot, fig=TRUE, height=7.5, width=7>>=
## visualization
trellis.par.set(theme = canonical.theme(color = FALSE))
mykey <- simpleKey(c("CvM", "max LM", "DM"), points = FALSE, lines = TRUE)
mykey$lines$lty <- c(2, 3, 1)
mykey$lines$lwd <- c(1.2, 2, 1.2)
print(xyplot(power ~ diff | pars + nobs, group = ~ test, data = mz_sim_sub,
  type = "l", lwd = c(1, 1, 2), ylim = c(0, 1), key = mykey,
  xlab = "Violation Magnitude", ylab = "Power"))
@
\end{figure}


\begin{table*}
\caption{Simulated power for three test statistics
  across four sample sizes $n$, nine magnitudes of measurement
  invariance violations, and two subsets of tested
  parameters $k^*$. 
  See Figure~\ref{fig:simres} for a visualization (using all 17~violation magnitudes).}
\label{tab:simres}
\begin{center}
\begin{tabular}{lrlrrrrrrrrr}
  \hline
   & & & \multicolumn{9}{l}{Violation Magnitude (SE)} \\
<<mz_sim-table, results=tex>>=
## labeling
levels(mz_sim_sub$pars) <- gsub("k* = ", "", levels(mz_sim_sub$pars), fixed = TRUE)
levels(mz_sim_sub$nobs) <- gsub( "n = ", "", levels(mz_sim_sub$nobs), fixed = TRUE)
levels(mz_sim_sub$test) <- c("$\\mathit{DM}$", "$\\mathit{CvM}$", "$\\max \\mathit{LM}$")

## table
mz_tab <- round(ftable(100 * xtabs(power ~ nobs + pars + test + diff,
  data = mz_sim_sub, subset = diff %in% c(seq(0, 4, by = 0.5))),
  col.vars = "diff"), digits = 1)
mz_tab <- format(mz_tab, quote = FALSE)[-2, -4]
mz_tab[1,] <- c("$n$", "$k^*$", "Statistic", format(seq(0, 4, by = 0.5)))
mz_tab <- paste(apply(mz_tab, 1, paste, collapse = " & "), "\\\\")
mz_tab[c(1, length(mz_tab))] <- paste(mz_tab[c(1, length(mz_tab))], "\\hline")
mz_tab <- c(mz_tab,"\\multicolumn{2}{l}{{\\scriptsize{Abbreviations:}}} &
\\multicolumn{10}{l}{{\\scriptsize{CvM = Cram\\'{e}r-von Mises 
  test; $\\max \\mathit{LM}$ = Maximum Lagrange multiplier test;}}} \\\\")
mz_tab <- c(mz_tab," & & \\multicolumn{10}{l}{{\\scriptsize{DM = Double-max test.}}}")
writeLines(mz_tab)
@

\end{tabular}
\end{center}
\end{table*}



\section{Application: Stereotype Threat}


\citeA{WicDol05}, henceforth WDH, utilized confirmatory
factor models to study measurement invariance in a series of
general intelligence scales.  They were interested in the
notion of {\em{stereotype threat}}, whereby stereotypes concerning a
subgroup's ability adversely impact the subgroup's performance on tests of that
ability.  The authors specifically focused on stereotypes concerning
ethnic minorities' performance on tests of general intelligence.  In this section, we use the proposed tests to supplement the authors' original analyses.

\subsection{Background}



To study stereotype threat in a measurement invariance framework, Study~1 of
\citeA{WicDol05} involved 295 high school students
completing three intelligence tests in two between-subjects conditions. 
Conditions were defined by whether or not students received primes about ethnic
stereotypes prior to testing. To study the data, WDH employed a
four-group, one-factor model with the three intelligence tests as manifest
variables. The groups were defined by ethnicity (majority/minority) and by the
experimental manipulation (received/did not receive stereotype prime). Results
indicated that the intelligence tests lacked measurement invariance, with the
minority group receiving stereotype primes being particularly different from the
other three groups on the most difficult intelligence test.  In the
example below, we employ one of the models used by WDH.
Our $V$ is participants' aptitudes, as measured by their grade point averages (GPAs).  The GPAs were unused in the original analyses.

\subsection{Method}
WDH tested a series of confirmatory factor models for
various types of measurement invariance.  We focus on the model used
in their Step~5b (see pp.~703--704 of their paper), which involved
across-group restrictions on the factor loadings, unique
variances, intercepts, and factor variances.  These parameters were
typically restricted to be equal across groups, though a subset of the
parameters were allowed to be group-specific upon examination of
modification indices.    The model
provided a reasonable fit to the data, as judged by examination of
$\chi^2$, RMSEA (root-mean-square error of approximation), and CFI
(comparative fit index) statistics.  While the model included four
groups as described above, we focus only on the submodel for the minority
group that received stereotype primes, which is pictured in Figure~\ref{fig:wpath}.  In the figure, paths with dashed lines and red parameters signify group-specific parameters.  These include the factor mean $\eta$, numerical factor loading $\lambda_{\text{num}}$, numerical intercept $\mu_{\text{num}}$, and numerical unique variance $\psi_{\text{num}}$.  Other parameters (not displayed) were restricted to be equal across all four groups, with the exception of the factor variance.  The factor variance was restricted to be equal within both minority groups, and separately within both majority groups.

\begin{figure}
\caption{Path diagram representing the submodel for the ``minority, stereotype'' group in model 5b of WDH.  Dashed paths and red parameters are group specific.  Other parameters (not displayed) were restricted to be equal across all four groups, with the factor variance only being restricted across the two minority groups.}
\label{fig:wpath}
\includegraphics[height=3.5in]{wicherts_msp.pdf}
\end{figure}

To carry out the tests, we first fit the four-group model to the data,
calculating casewise derivatives and  the observed information matrix.   Second,
to assess measurement invariance within the ``minority, stereotype prime'' group
only, the scores of the $n_{\text{MSP}}$ individuals from that group are ordered
and aggregated along GPA (i.e.,  in this application the variable $V$). The
cumulative score process \eqref{eq:cumscore} with respect to GPA then allows us
to obtain various test statistics and $p$-values from this  process.  Test
statistics include the double-max statistic from \eqref{eq:dmax}, the
Cram\'{e}r-von Mises statistic from \eqref{eq:cvm}, and the $\max \mathit{LM}$
statistic from \eqref{eq:maxlm}.  We focus on the double-max statistic
due to its ease of interpretation and intuitive visual display.

As mentioned in the theory section, the 
tests give us the flexibility to study hypotheses of partial change: we
have the ability to  test various subsets of  parameters.  For the WDH data, we
first test
\begin{equation}
    \label{eq:exh0}
H_0:\ (\lambda_{i, \text{num}}\ \eta_{i}\ \mu_{i, \text{num}}\ \psi_{i, \text{num}}) = 
      (\lambda_{0, \text{num}}\ \eta_{0}\ \mu_{0, \text{num}}\ \psi_{0, \text{num}}),\
      i = 1,\ldots, n_{\text{MSP}},
\end{equation}
where
$\lambda_{i, \text{num}}$ is the factor loading on the numerical scale,
$\eta_{i}$ is student $i$'s factor mean,
$\mu_{i, \text{num}}$ is student $i$'s intercept on the numerical scale, and
$\psi_{i, \text{num}}$ is student $i$'s unique variance on the numerical scale.
Thus, \eqref{eq:exh0} states  that these four group-specific
model parameters are invariant with respect to GPA.



\subsection{Results}

<<wdh_data>>=
## data and transformations
data("StereotypeThreat", package = "psychotools")
## include group variable
StereotypeThreat <- transform(StereotypeThreat, group = interaction(ethnicity, condition))
StereotypeThreat <- StereotypeThreat[order(StereotypeThreat$group),]
## omit NAs in GPA and break ties randomly
StereotypeThreat <- subset(StereotypeThreat, !is.na(gpa))
ties <- duplicated(StereotypeThreat$gpa)
set.seed(1090)
StereotypeThreat$gpa[ties] <- runif(sum(ties),
  min = StereotypeThreat$gpa[ties] - 0.001, max = StereotypeThreat$gpa[ties] + 0.01)
## keep only data without NAs in GPA
wdh <- subset(StereotypeThreat, !is.na(gpa))
@

<<wdh_model>>=
## fit model 5b
wdh5b <- lavaan(
   'ability   =~ label(c(rep("load_n", 3), "load_n:min_t")) * numerical + label(rep("load_v", 4)) * verbal + 1 * abstract
    ability   ~  label(c(NA, "lmean:min_c", "lmean:maj_t", "lmean:min_t")) * 1 + c(0, NA, NA, NA) * 1
    ability   ~~ label(c("lvar:maj", "lvar:min", "lvar:maj", "lvar:min")) * ability
    numerical ~  label(c(rep("mean_n", 3), "mean_n:min_t")) * 1
    abstract  ~  label(c("mean_a:maj_c", "mean_a:min_c", rep("mean_a", 2))) * 1
    verbal    ~  label(rep("mean_v", 4)) * 1
    numerical ~~ label(c(rep("var_n", 3), "var_n:min_t")) * numerical
    abstract  ~~ label(rep("var_a", 4)) * abstract
    verbal    ~~ label(rep("var_v", 4)) * verbal',
  data = wdh, meanstructure = TRUE, group = "group") ##, likelihood = "wishart")

## refit model with new identification constraints
wdh5b_1 <- lavaan(
   'ability   =~ 1 * verbal + label(c(rep("load_n", 3), "load_n:min_t")) * numerical  + label(rep("load_a", 4)) * abstract
    ability   ~  label(c(NA, "lmean:min_c", "lmean:maj_t", "lmean:min_t")) * 1 + c(0, NA, NA, NA) * 1
    ability   ~~ label(c("lvar:maj", "lvar:min", "lvar:maj", "lvar:min")) * ability
    numerical ~  label(c(rep("mean_n", 3), "mean_n:min_t")) * 1
    abstract  ~  label(c("mean_a:maj_c", "mean_a:min_c", rep("mean_a", 2))) * 1
    verbal    ~  label(rep("mean_v", 4)) * 1
    numerical ~~ label(c(rep("var_n", 3), "var_n:min_t")) * numerical
    abstract  ~~ label(rep("var_a", 4)) * abstract
    verbal    ~~ label(rep("var_v", 4)) * verbal',
  data = wdh, meanstructure = TRUE, group = "group") #, likelihood = "wishart")
@ 

The double-max test for the hypothesis~\eqref{eq:exh0} is shown in
Figure~\ref{fig:cusumgpa}, displaying the cumulative score processes
for each of the four group-specific parameters across GPA along
with horizontal lines representing the critical value for $\alpha= 0.05$.
This shows that the ``minority, stereotype prime'' group lacks invariance
with respect to GPA,
because one of the processes crosses its boundaries. Furthermore, as this
process pertains to the variance $\psi_\text{num}$ of the numerical scale
(bottom panel) it can be concluded that the invariance violation is associated
with this parameter.  Both the factor loading for the numerical scale
$\lambda_\text{num}$ and the factor mean $\eta$ also exhibit increased fluctuation, 
although they are just non-significant at level $\alpha = 0.05$. Only the process
associated with the intercept $\mu_\text{num}$ shows moderate (and hence
clearly non-significant) fluctuation across GPA.

<<gefp-4-1>>=
info_full <- function(x, ...) solve(vcov(x) * nobs(x))
scores_min_t <- function(x, ...) {
  ef <- estfun(x, ...)
  ef[wdh$group == "minority.threat", ]
}
gpa_min_t <- subset(wdh, group == "minority.threat")$gpa

## empirical fluctuation processes (4 vs 1 par)
gefp_4_info <- gefp(wdh5b, fit = NULL, scores = scores_min_t, order.by = gpa_min_t, vcov = info_full, sandwich = FALSE, parm = 15:18)
gefp_1_info <- gefp(wdh5b, fit = NULL, scores = scores_min_t, order.by = gpa_min_t, vcov = info_full, sandwich = FALSE, parm = 18)

# processes for second identification constraints
gefp_4_info_1 <- gefp(wdh5b_1, fit = NULL, scores = scores_min_t, order.by = gpa_min_t, vcov = info_full, sandwich = FALSE, parm = 15:18)
gefp_1_info_1 <- gefp(wdh5b_1, fit = NULL, scores = scores_min_t, order.by = gpa_min_t, vcov = info_full, sandwich = FALSE, parm = 18)
@

\begin{figure}
\centering
\caption{Cumulative score processes for the four group-specific parameters across GPA
  in the ``minority, stereotype prime'' group
  using data from Study 1 of Wicherts et al.\ (2005). %% \citeA{WicDol05} does not work in caption
  The solid gray horizontal lines correspond to the critical value of
  the doule-max test at $\alpha = 0.05$.}
\label{fig:cusumgpa}
\setkeys{Gin}{width=0.8\textwidth}
<<gefp-4-plot, fig=TRUE, height=8, width=5.5>>=
gefp_4_bound <- get_boundary(gefp_4_info, maxBB)
plot(gefp_4_info$process,
  ylim = c(-2.15, 2.15), main = "DM, k* = 4", xlab = "GPA",
  ylab = expression(lambda[num], eta, mu[num], psi[num]),
  panel = function(x, y, ...) {
    abline(h = 0)
    lines(gefp_4_bound, col = "slategray", lwd = 1.7)
    lines(-gefp_4_bound, col = "slategray", lwd = 1.7)
    lines(x, y, ...)
  })

@
\end{figure}

We can also use the test results to locate the threshold $\nu$,
which in this context can be used to define GPA-based subgroups whose
measurement parameters differ.  As described previously, information about
the location of $\nu$ can be obtained by 
examining the peaks in Figure~\ref{fig:cusumgpa}. The main peak occurs
around a GPA of~6.3 and a second one near~5.9
(6~corresponds to the American `C' range).  These peaks roughly divide
individuals into those receiving D's and F's, those receiving solid
C's, and those receiving high C's, B's, and A's.

In addition to the double-max test, the Cram\'er-von Mises or $\max \mathit{LM}$
statistics could also be employed. They also lead to clearly significant
violations of measurement invariance ($p < 0.005$ for both tests) and
the corresponding visualizations bring out similar
peaks as the double-max test in Figure~\ref{fig:cusumgpa} (and hence
are omitted here).

\subsection{Summary}
Application of the proposed measurement invariance tests to the
\citeA{WicDol05} data allowed us to study the extent to which model
parameters are invariant with respect to GPA in a straightforward manner.
The tests focused on a set of group-specific
parameters within a four-group confirmatory factor model, and they
could be carried out using the results from a single estimated model.
The latter fact is important, as other approaches to the problem
described here may require multiple models to
be estimated (e.g., for LR tests) or adversely impact model fit and
degrees of freedom (e.g., if GPA is inserted directly into the model).
Further, through examination of both 
test statistics and cumulative score processes, the tests were
interpretable from both a theoretical and applied standpoint.

We included a factor mean in the application in order to
demonstrate the tests' generality.  To deal with 
measurement invariance specifically, however, we may elect to focus on
only the 
measurement parameters within the model (omitting the factor mean).
That is, it would 
not have been notable or surprising if we had observed that
the factor mean lacked measurement invariance with respect to GPA:
this result would have implied that individuals' latent
intelligence fluctuates with GPA.  We speculate that such a result
would often be obtained when $V$ is related to the latent variable of
interest.


\section{General Discussion}
In this paper, we have applied a family of stochastic process-based
statistical tests to  
the study of measurement invariance in psychometrics.  The tests
have reasonable power, can isolate
subgroups of individuals violating measurement invariance based on a
continuous auxiliary variable, and can
isolate specific model parameters affected by the violation.  In this
section, we consider the tests' use in practice and their 
extension to more complex scenarios.

<<constraint_study>>=
tests <- list(maxBB, meanL2BB, supLM(0.1))
p.diffs <- matrix(NA, nrow = 2 * length(tests), ncol = 2)
for (i in 1:length(tests)){
  p.diffs[                i, 1] <- sctest(gefp_4_info,   functional = tests[[i]])$p.value
  p.diffs[length(tests) + i, 1] <- sctest(gefp_1_info,   functional = tests[[i]])$p.value
  p.diffs[                i, 2] <- sctest(gefp_4_info_1, functional = tests[[i]])$p.value
  p.diffs[length(tests) + i, 2] <- sctest(gefp_1_info_1, functional = tests[[i]])$p.value
}
@ 

\subsection{Use in Practice}
The proposed tests give researchers a new set of tools for studying 
measurement invariance, allowing them the 
flexibility to: (1)~simultaneously test all model parameters across
all individuals, yielding
results relevant to many types of measurement
invariance (see, e.g., \citeNP{Mer93}), (2)~test a subset of
model parameters, either across all individuals or within a
multiple-group model, and (3)~use the tests as a type of modification
index following significant LR~tests.
Traditional steps to studying measurement invariance have involved
sequential LR~tests for various types of invariance using
multiple-group models.  As shown in the ``Application'' section, it can be
beneficial to employ the traditional steps in tandem with the proposed
tests.  Furthermore, when groups are not defined in advance (e.g.,
continuous $V$), the traditional steps fail unless further assumptions
about the nature of the groups are made.

It is worth mentioning that the proposed tests are not invariant
with respect to the choice of identification constraints. They share this
property with both LM and Wald tests (while LR tests are invariant to
the choice of identification constraints). However, asymptotically
the influence of the parametrization disappears and also in finite
samples it typically does not change the results significantly.
For example, in the stereotype threat application, we assessed the p-values
of all three tests under the constraint that the abstract reasoning
loading is fixed to 1 (see Figure~\ref{fig:wpath}) and compared them to the p-values
under the constraint that the verbal loading is fixed to 1. All p-values
remained clearly significant and the changes were all smaller than
\Sexpr{format(round(max(abs(p.diffs[1:3,2] - p.diffs[1:3,1])), digits = 5), scientific = FALSE)}.

In addition to providing general information about whether or not measurement 
invariance holds, the proposed tests allow researchers to interpret
the nature of the invariance violation.  This is made possible, e.g.,
through the tests' abilities to locate $\nu$, the threshold dividing
individuals into subgroups that violate
measurement invariance (see Equation~\eqref{eq:h1*}).  It is also
possible to formally estimate one or more $\nu$ thresholds by
adopting a (partially) segmented model (see e.g., \citeNP{ZeiSha10}).



\subsection{Comparison to Other Methods}

The step functions from~\eqref{eq:h1*} were employed in the current paper to
highlight connections with nested model
comparison, but the proposed tests typically have non-trivial power for
all (non-constant) deviation patterns $\theta(v_i)$ (considered in
Equation~\eqref{eq:h1}). Thus, the tests will also have power for linear
deviations from parameter constancy, although other techniques may 
perform better in this particular case.  Examples include the 
class of moderated factor models \cite{BauHus09,MolDol10,Pur02}, 
whereby continuous moderators are allowed to linearly impact model
parameters (Purcell also considers quadratic effects of the
moderators). Moreover, if the change in
parameters is continuous but not exactly linear -- e.g., a sigmoidal
shift from one set of parameters to another one -- then the simple
single shift model in~\eqref{eq:h1*} may be a useful first
approximation and may have better power than linear techniques
(depending on how clearly the two regimes are separated). 

The proposed tests are also similar to factor mixture models (e.g.,
\citeNP{DolMaa98,LubMut05}) in that they can handle unknown subgroups
violating measurement invariance. Covariates can also be used to
predict the unknown subgroups under both approaches. For example, for the 
data used in our simulations, one could employ a factor mixture
model with two latent classes where the class probabilities depend
on student's age through a logit link. If the assumptions of such
a mixture hold, i.e., that each observation is a weighted combination
from a (known) number of classes, the fitted model has more power
uncovering this structure and makes it easy to interpret. However,
the proposed tests are easier to use with fewer assumptions about
the type of deviations (as long as they occur along the selected
variable $V$).


More generally, all the approaches described above can be distinguished
from the proposed tests in that they require estimation of a new model
of increased complexity (due to the moderator/covariate).
The proposed tests, on the other hand, are of the
``posthoc'' variety, relying only on results calculated during the
original model estimation.  While no method clearly dominates 
across all situations, use of the proposed tests can at least reduce 
some of the technical issues associated with estimating and
interpreting models of greater complexity.  This alone is not a
good reason to use the tests, but it is a consideration that is often
meaningful in practice.


\subsection{Categorical Auxiliary Variable}
One issue that was largely unaddressed in this paper involved the use
of categorical $V$ to study measurement invariance.  In this case,
groups are 
already specified in advance, and so traditional methods for fixed
subgroups (i.e., LR, Wald, and LM tests) may suffice.
However, we can also obtain an LM-type statistic from the
framework developed here. Assume the observations are divided into
$C$ categories $I_1, I_2, \ldots, I_C$. Then, the increment of
the cumulative score process $\Delta_{I_c} {\bm B}(\hat {\bm \theta})$
within each category is just the sum of the corresponding scores.
In somewhat sloppy notation:
\begin{equation}
    \label{eq:catsum}
    \Delta_{I_c} {\bm B}(\hat {\bm \theta}) ~=~ \hat {\bm I}^{-1/2} n^{-1/2}
    \sum_{i \in I_c} {\bm s}(\hat {\bm \theta}; x_i).
%% \\
%%    X^2 & = & \sum_{c = 1}^C n_c^{-1} n^{-1} || \Delta_{I_c} {\bm B}(\hat {\bm \theta}) ||^2
\end{equation}
This results in a $C \times k$ matrix, with one entry for
each category-by-model parameter combination.  We can test
a specific model parameter for invariance by focusing on the associated
column of the $C \times k$ matrix and employing a weighted squared sum of
the entries in the column to obtain a $\chi^2$-distributed statistic with
$(C-1)$ degrees of freedom \cite{HjoKon02}.
Alternatively, to simultaneously test multiple parameters, we can sum
the $\chi^2$ statistics and degrees of freedom for the individual
parameters.  In addition to categorical $V$, this framework may 
be useful for ordinal $V$ (or continuous $V$ with many ties).
In this situation, one could also adapt the statistics
\eqref{eq:dmax}, \eqref{eq:cvm}, and \eqref{eq:maxlm} computed from the same cumulative score
process as usual. The only required modification is that the statistics
should not depend on the process's values ``within'' a category (or group
of ties). This is easily achieved by taking the maximum (or sum) not over
all observations $i = 1, \dots, n$, but over only those $i$ at the ``end'' of a
category.
%% If $i_j$ is the cumulative frequency of category $j$ or lower
%% (with $j = 1, \dots, C$), then one could take the outer maximum in
%% \eqref{} over $i = i_1, i_2, \dots, i_{C-1}$.


The number of potential thresholds may be very low in these
situations, which impacts the extent to which asymptotic results hold
for the main test statistics described in this paper.

\subsection{Extensions}
The family of tests described in this paper can be extended in various
ways.  First, 
it is possible to construct an algorithm that recursively defines
groups of individuals violating measurement invariance with respect to
multiple auxiliary variables.  Such an algorithm is related to 
classification and regression trees \cite{BreFri84,MerSha10,StrMal09},
with related algorithms being developed for general parametric models
\cite{ZeiHot08} and Rasch models in particular \cite{KopZei10}.  

Relatedly, \citeA{San09} describes a general
method for partitioning/segmenting structural equation models within a 
partial least squares framework.  This method involves direct maximization of
the likelihood ratio (i.e., fitting the model for various subgroups
defined by 
$V$ and choosing the subgroups with the largest likelihood ratio).
Thus, unlike the tests described in this paper, this approach does not
provide a formal significance test with a controlled level of type~I errors.

The proposed tests also 
readily extend to other psychometric models.  
For example, the tests can be used to generally 
study the stability of structural equation model parameters across
observations.  This includes the study of measurement invariance
in second-order growth models and related models for longitudinal
data (e.g., \citeNP{FerBal08,McA09}).
The issue of measurement invariance is important in these models in
order to establish that a ``true'' change has occurred in the 
individuals to which the model has been fitted.  The tests described
here can be used to help establish measurement invariance with respect
to both continuous and categorical auxiliary variables, using only
output from the estimated model of interest.  

Finally, the tests can be used for the study of differential
item functioning (DIF) 
in item response models (e.g., \citeNP{KopZei10}, who focused on Rasch
models).   
Traditional DIF methods are similar to those for factor analysis in
that subgroups must be specified in advance.
The tests proposed here can detect subgroups automatically, however, offering a unified way of
studying both  measurement invariance in factor analysis and differential item
functioning in item response.
While factor-analytic measurement invariance methods and DIF methods
have developed 
largely independently of one another, the methods can certainly be
treated from a unified perspective (e.g.,
\citeNP{Mcd99,Mil11,StaChe06}).   
The tests proposed here were designed with this perspective in mind.


\subsection{Summary}
We outline a family of 
stochastic process-based parameter instability tests from theoretical
statistics and apply them to the issue of measurement invariance in psychometrics.
The paper includes theoretical development, an applied example, and 
study of the tests' performance under controlled conditions.  The tests are found to have good
properties via simulation, making them useful for many psychometric
applications.  More generally, the tests help solve standing problems in
measurement invariance research and provide many avenues for
future research.  This can happen both through extensions of the tests
within a factor-analytic context and through application of the tests to new
models.

\section*{Computational Details}

All results were obtained using the {R}~system for statistical computing \cite{R11},
version~\Sexpr{paste(R.Version()[6:7], collapse = ".")}, employing the add-on packages
{lavaan}~\Sexpr{packageDescription("lavaan")["Version"]} \cite{lavaan11} and
{OpenMx}~\Sexpr{packageDescription("OpenMx")["Version"]} \cite{BokNea11} for fitting of the factor analysis models and
{strucchange}~\Sexpr{packageDescription("strucchange")["Version"]} \cite{ZeiLei02,Zei06}
for evaluating the parameter instability tests. {R}~and the packages {lavaan} and {strucchange} are
freely available under the General Public License~2 from the
Comprehensive {R} Archive Network at \url{http://CRAN.R-project.org/}
while {OpenMx} is available under the
Apache License~2.0 from \url{http://OpenMx.psyc.virginia.edu/}.
{R}~code for replication of our results is
available at \url{http://semtools.R-Forge.R-project.org/}.


\bibliography{refs}

\end{document}
